{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "In Natural Language Processing (NLP) an Entity Recognition is one of the common problem. The entity is referred to as the part of the text that is interested in. In NLP, NER is a method of extracting the relevant information from a large corpus and classifying those entities into predefined categories such as location, organization, name and so on. \n",
    "Information about lables: \n",
    "* geo = Geographical Entity\n",
    "* org = Organization\n",
    "* per = Person\n",
    "* gpe = Geopolitical Entity\n",
    "* tim = Time indicator\n",
    "* art = Artifact\n",
    "* eve = Event\n",
    "* nat = Natural Phenomenon\n",
    "\n",
    "        1. Total Words Count = 1354149 \n",
    "        2. Target Data Column: Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wt4u0Lf1YJPH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D, ZeroPadding2D\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmFVrk0JY-Mv"
   },
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "file = \"C:/Users/Admin/Desktop/NLP/Sentimental/ner_dataset.csv\"\n",
    "df = pd.read_csv(file, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47959</td>\n",
       "      <td>1048575</td>\n",
       "      <td>1048575</td>\n",
       "      <td>1048575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>47959</td>\n",
       "      <td>35178</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sentence: 2212</td>\n",
       "      <td>the</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>52573</td>\n",
       "      <td>145807</td>\n",
       "      <td>887908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sentence #     Word      POS      Tag\n",
       "count            47959  1048575  1048575  1048575\n",
       "unique           47959    35178       42       17\n",
       "top     Sentence: 2212      the       NN        O\n",
       "freq                 1    52573   145807   887908"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations : \n",
    "* There are total 47959 sentences in the dataset.\n",
    "* Number unique words in the dataset are 35178.\n",
    "* Total 17 lables (Tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\n",
       "       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\n",
       "       'I-eve', 'I-nat'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the unique Tags\n",
    "df['Tag'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of missing values in 'Sentence #' attribute. So we will use pandas fillna technique and use 'ffill' method which propagates last valid observation forward to next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PTsjCdBZ9Xy"
   },
   "outputs": [],
   "source": [
    "df = df.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9DYzRvMagm5"
   },
   "outputs": [],
   "source": [
    "# This is a class te get sentence. The each sentence will be list of tuples with its tag and pos.\n",
    "class sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                       s['POS'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Sentence #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying one full sentence\n",
    "getter = sentence(df)\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "#sentence with its pos and tag.\n",
    "sent = getter.get_text()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the sentences in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3F0_tiOmaiVi"
   },
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the parameters for CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRQJJSoyamU4"
   },
   "outputs": [],
   "source": [
    "# Number of data points passed in each iteration\n",
    "batch_size = 64 \n",
    "# Passes through entire dataset\n",
    "epochs = 8\n",
    "# Maximum length of review\n",
    "max_len = 75 \n",
    "# Dimension of embedding vector\n",
    "embedding = 40 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Data\n",
    "We will process our text data before feeding to the network.\n",
    "* Here word_to_index dictionary used to convert word into index value and tag_to_index is for the labels. So overall we represent each word as integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32qpbWMVau_5"
   },
   "outputs": [],
   "source": [
    "#Getting unique words and labels from data\n",
    "words = list(df['Word'].unique())\n",
    "tags = list(df['Tag'].unique())\n",
    "# Dictionary word:index pair\n",
    "# word is key and its value is corresponding index\n",
    "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
    "word_to_index[\"UNK\"] = 1\n",
    "word_to_index[\"PAD\"] = 0\n",
    "\n",
    "# Dictionary lable:index pair\n",
    "# label is key and value is index.\n",
    "tag_to_index = {t : i + 1 for i, t in enumerate(tags)}\n",
    "tag_to_index[\"PAD\"] = 0\n",
    "\n",
    "idx2word = {i: w for w, i in word_to_index.items()}\n",
    "idx2tag = {i: w for w, i in tag_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tcC_UuUbav7y"
   },
   "outputs": [],
   "source": [
    "# Converting each sentence into list of index from list of tokens\n",
    "X = [[word_to_index[w[0]] for w in s] for s in sentences]\n",
    "\n",
    "# Padding each sequence to have same length  of each word\n",
    "X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-C7iFNjaytc"
   },
   "outputs": [],
   "source": [
    "# Convert label to index\n",
    "y = [[tag_to_index[w[2]] for w in s] for s in sentences]\n",
    "\n",
    "# padding\n",
    "y = pad_sequences(maxlen = max_len, sequences = y, padding = \"post\", value = tag_to_index[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbnAi9kwa0gL"
   },
   "outputs": [],
   "source": [
    "num_tag = df['Tag'].nunique()\n",
    "# One hot encoded labels\n",
    "y = [to_categorical(i, num_classes = num_tag + 1) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmj_9AzCa23d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training input data :  (40765, 75)\n",
      "Size of training output data :  (40765, 75, 18)\n",
      "Size of testing input data :  (7194, 75)\n",
      "Size of testing output data :  (7194, 75, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training input data : \", X_train.shape)\n",
    "print(\"Size of training output data : \", np.array(y_train).shape)\n",
    "print(\"Size of testing input data : \", X_test.shape)\n",
    "print(\"Size of testing output data : \", np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(539550, 18)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.asarray(y_train)\n",
    "m=y.size\n",
    "y = y.reshape((m))\n",
    "y = y.reshape((-1,18))\n",
    "\n",
    "Y_test=np.asarray(y_test)\n",
    "n=Y_test.size\n",
    "Y_test = Y_test.reshape((n))\n",
    "Y_test = Y_test.reshape((-1,18))\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2791,
     "status": "ok",
     "timestamp": 1560703209499,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "6WRJfQ5ca4vD",
    "outputId": "a908468a-3b1f-4680-afe7-6cc22a8a9394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 75)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 75, 75)            2638500   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 75, 75)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 75, 300)           67800     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 75, 150)           135150    \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 75, 75)            33825     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 75, 18)            1368      \n",
      "=================================================================\n",
      "Total params: 2,876,643\n",
      "Trainable params: 2,876,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D,Bidirectional\n",
    "from keras.layers import MaxPool1D\n",
    "# Model architecture\n",
    "filters=300\n",
    "kernel_size=3\n",
    "input = Input(shape = (max_len,))\n",
    "model = Embedding(input_dim = len(words) + 2, output_dim = max_len, input_length = max_len, mask_zero = True)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Conv1D(filters, kernel_size, padding='same', activation='relu', strides=1)(model)\n",
    "model = Conv1D(150, kernel_size, padding='same', activation='relu', strides=1)(model)\n",
    "model = Conv1D(75, kernel_size, padding='same', activation='relu', strides=1)(model)\n",
    "# model =Flatten()(model)\n",
    "# model=MaxPool1D(pool_size = 2)(model)\n",
    "out = Dense(num_tags+1,activation = 'softmax')(model)\n",
    "model = Model(input,out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Checkpoint each epoch to check and save the best model performance till last and also avoiding further validation loss drop due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2003225,
     "status": "ok",
     "timestamp": 1560708147077,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "SjKhhXHMG-jJ",
    "outputId": "bd461b08-3920-4920-c3a6-10eb8f3cf432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 261s 451ms/step - loss: 0.0251 - accuracy: 0.9577 - val_loss: 0.0042 - val_accuracy: 0.9876\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, np.asarray(y_train),  batch_size = 64, verbose = 1, epochs = 1, validation_split = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1054,
     "status": "ok",
     "timestamp": 1560705868376,
     "user": {
      "displayName": "CHAVAN AKSHAY",
      "photoUrl": "",
      "userId": "10674464813829582221"
     },
     "user_tz": -330
    },
    "id": "vEsREje5ubq-",
    "outputId": "083b774d-ae35-4720-e515-7930617364d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_input(text):\n",
    "    word_list = text.split(\" \")\n",
    "    x_new = []\n",
    "    for word in word_list:\n",
    "        x_new.append(word_to_index[word])\n",
    "    d=np.array([x_new])\n",
    "    g = pad_sequences(maxlen = max_len, sequences = d, padding = \"post\", value = word_to_index[\"PAD\"])\n",
    "\n",
    "    p = model.predict(np.array(g))\n",
    "    p = np.argmax(p, axis = -1)\n",
    "    print(\"{:20}\\t{}\\n\".format(\"Word\", \"Prediction\"))\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    for (w, pred) in zip(range(len(x_new)), p[0]):\n",
    "        print(\"{:20}\\t{}\".format(word_list[w], idx2tag[pred]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                \tPrediction\n",
      "\n",
      "-----------------------------------\n",
      "my                  \tO\n",
      "friend              \tO\n",
      "Mohammed            \tB-per\n",
      "is                  \tO\n",
      "travelling          \tO\n",
      "to                  \tO\n",
      "London              \tB-geo\n",
      "are                 \tO\n"
     ]
    }
   ],
   "source": [
    "test_inputs = \"my friend Mohammed is travelling to London are\"\n",
    "test_input(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                \tPrediction\n",
      "\n",
      "-----------------------------------\n",
      "Thousands           \tO\n",
      "of                  \tO\n",
      "demonstrators       \tO\n",
      "have                \tO\n",
      "marched             \tO\n",
      "through             \tO\n",
      "London              \tB-geo\n",
      "to                  \tO\n",
      "protest             \tO\n",
      "the                 \tO\n",
      "war                 \tO\n",
      "in                  \tO\n",
      "Iraq                \tB-geo\n",
      "and                 \tO\n",
      "demand              \tO\n",
      "the                 \tO\n",
      "withdrawal          \tO\n",
      "of                  \tO\n",
      "British             \tB-gpe\n",
      "troops              \tO\n",
      "from                \tO\n",
      "that                \tO\n",
      "country             \tO\n"
     ]
    }
   ],
   "source": [
    "test_inputs = \"Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country\"\n",
    "test_input(test_inputs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
